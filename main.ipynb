{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92e3da-3717-4f30-a393-8a8828f06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import importlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from autumn.notebook import *\n",
    "from autumn.math import *\n",
    "from autumn.images import *\n",
    "from autumn.guidance import *\n",
    "from autumn.scheduling import *\n",
    "from autumn.prompting import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a33fb-7751-45d9-a67b-3d7da6ca52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%settings\n",
    "\n",
    "# model can be a local path or a huggingface model name\n",
    "model = \"stabilityai/stable-diffusion-xl-base-0.9\"\n",
    "XL_MODEL = True # this notebook in its current state is only really guaranteed to work with SDXL\n",
    "vae_scale = 0.13025 # found in the model config\n",
    "\n",
    "run_ids = range(1)\n",
    "\n",
    "# run context will contain only context.run_id, anything returned in this dictionary will be added to it\n",
    "add_run_context = lambda context: {}\n",
    "\n",
    "p0 = \"photograph of a very cute dog\"\n",
    "np0 = \"blurry, ugly, indistinct, jpeg artifacts, watermark, text, signature\"\n",
    "\n",
    "# Generally: e2_prompt has the strongest effect, but terms from e1_prompt will show up in results.\n",
    "# e2_pool_prompts has an influence but very indirect idk\n",
    "# the model is trained w/ all encodings coming from the same prompt\n",
    "\n",
    "prompts = lambda context: {\n",
    "    # Prompts for encoder 1\n",
    "    \"encoder_1\": [np0, p0],\n",
    "    # Prompts for encoder 2; defaults to e1_prompts if None\n",
    "    \"encoder_2\": None,\n",
    "    # Prompts for pooled encoding of encoder 2; defaults to e2_prompts if None\n",
    "    \"encoder_2_pooled\": None\n",
    "}\n",
    "\n",
    "# Method by which predictions for different prompts will be recombined to make one noise prediction\n",
    "combine_predictions = lambda context: scaled_CFG(\n",
    "    difference_scales = [\n",
    "        (1, 0, sigmoid(0.1 * vae_scale, 3 * vae_scale))\n",
    "    ], \n",
    "    steering_scale = id_, base_scale = id_, total_scale = id_\n",
    ")\n",
    "\n",
    "seed = lambda context: 42069\n",
    "\n",
    "# these get multiplied by 64\n",
    "width_height = lambda context: (16, 16)\n",
    "\n",
    "steps = lambda context: 25\n",
    "\n",
    "# step context will contain run_id & step_index, anything returned in this dictionary will be added to it\n",
    "add_step_context = lambda context: {}\n",
    "\n",
    "#distort = sigmoid(0.319, 0.987, 0.0304, 0.025)\n",
    "#distort = scale_f(distort, 127, 1.0)\n",
    "embedding_distortion = lambda context: None\n",
    "\n",
    "dynamic_thresholding = lambda context: False\n",
    "dynthresh_percentile = lambda context: 0.995\n",
    "dynthresh_target = lambda context: 7\n",
    "\n",
    "naive_rescaling = lambda context: False\n",
    "\n",
    "save_output = lambda context: True\n",
    "save_approximates = lambda context: False\n",
    "save_raw = lambda context: False\n",
    "\n",
    "#!# Settings above this line will be replaced with the contents of settings.py if it exists. #!#\n",
    "\n",
    "main_dtype = torch.float64\n",
    "unet_dtype = torch.float16\n",
    "vae_dtype = torch.float32\n",
    "encoder_dtype = torch.float16\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31d118-c74d-4693-a1d3-6f6383e854c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # # models # # #\n",
    "\n",
    "model_source = model\n",
    "\n",
    "with Timer(\"total\"):\n",
    "    with Timer(\"vae\"):\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            model_source, subfolder=\"vae\", torch_dtype=vae_dtype\n",
    "        )\n",
    "        vae.to(device=\"cuda\")\n",
    "\n",
    "        # (as of torch 2.2.2) VAE compilation yields negligible gains, at least for decoding once/step\n",
    "        #vae = torch.compile(vae, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"unet\"):\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_source, subfolder=\"unet\", torch_dtype=unet_dtype\n",
    "        )\n",
    "        unet.to(device=\"cuda\")\n",
    "    \n",
    "        # compilation will not actually happen until first use of unet\n",
    "        # (as of torch 2.2.2) \"default\" provides the best result on my machine\n",
    "        # don't use this if you're gonna be changing resolutions a lot\n",
    "        unet_c = torch.compile(unet, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"clip\"):\n",
    "        p_encoder = PromptEncoder(model_source, XL_MODEL, torch_dtype=encoder_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24aeca-ec13-41cd-ae56-4a72c78dbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run # # #\n",
    "\n",
    "variance_range = (0.00085, 0.012) # should come from model config!\n",
    "forward_noise_schedule = default_variance_schedule(variance_range).to(main_dtype) # beta\n",
    "forward_signal_product = torch.cumprod((1 - forward_noise_schedule), dim=0) # alpha_bar\n",
    "partial_signal_product = lambda s, t: torch.prod((1 - forward_noise_schedule)[s+1:t]) # alpha_bar_t / alpha_bar_s (but computed more directly from the forward noise)\n",
    "part_noise = (1 - forward_signal_product).sqrt() # sigma\n",
    "part_signal = forward_signal_product.sqrt() # mu?\n",
    "\n",
    "def step_by_noise(latents, noise, from_timestep, to_timestep):\n",
    "    if from_timestep < to_timestep: # forward\n",
    "        signal_ratio = 1 / partial_signal_product(from_timestep, to_timestep).sqrt()\n",
    "    else: # backward\n",
    "        signal_ratio = partial_signal_product(to_timestep, from_timestep).sqrt()\n",
    "    return latents / signal_ratio + noise * (part_noise[to_timestep] - part_noise[from_timestep] / signal_ratio)\n",
    "\n",
    "def noisiness(at_timestep):\n",
    "    signal_ratio = partial_signal_product(at_timestep, 999).sqrt()\n",
    "    return part_noise[999] - part_noise[at_timestep] / signal_ratio\n",
    "\n",
    "def report_passthrough(c, x, s):\n",
    "    hist = torch.histogram(x.float().cpu() / vae_scale, bins=bins*s).hist\n",
    "\n",
    "    width = (len(bins) + 2) * 5\n",
    "    height = 100\n",
    "    plot = torch.ones([height, width])\n",
    "    print(f\"max {hist.max()}\")\n",
    "    hist /= hist.max()\n",
    "    \n",
    "    for i in range(len(bins) - 1):\n",
    "        bottom = height - 11\n",
    "        top = height - (int((height - 21) * (hist[i].item()) + 11))\n",
    "        left = 5 * (i + 1) + 2\n",
    "        right = 5 * (i + 1) + 4\n",
    "        plot[top:bottom,left:right] = 0\n",
    "\n",
    "    plot[:,width//2] = 0.5\n",
    "    plot[height-10,:] = 0.5\n",
    "    \n",
    "    mshow(plot)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "for run_id in run_ids:\n",
    "    run_context = Context()\n",
    "    run_context.run_id = run_id\n",
    "    add_run_context(run_context)\n",
    "    \n",
    "    try:\n",
    "        _seed = int(seed(run_context))\n",
    "    except:\n",
    "        _seed = 0\n",
    "        print(f\"non-integer seed, run {run_id}. replaced with 0.\")\n",
    "    \n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "\n",
    "    run_context.steps = steps(run_context)\n",
    "\n",
    "    diffusion_timesteps = linspace_timesteps(run_context.steps, timestep_max(run_context), timestep_min(run_context), timestep_power(run_context))\n",
    "\n",
    "    run_prompts = prompts(run_context)\n",
    "    \n",
    "    unet_batch_size = len(run_prompts[\"encoder_1\"])\n",
    "    \n",
    "    (all_penult_states, enc2_pooled) = p_encoder.encode(run_prompts[\"encoder_1\"], run_prompts[\"encoder_2\"], run_prompts[\"encoder_2_pooled\"])\n",
    "    \n",
    "    if embedding_distortion(run_context) is not None:\n",
    "        all_penult_states = svd_distort_embeddings(all_penult_states.to(torch.float32), embedding_distortion(run_context)).to(torch.float16)\n",
    "\n",
    "    width, height = width_height(run_context)\n",
    "\n",
    "    if (width < 64): width *= 64\n",
    "    if (height < 64): height *= 64\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vae_dim_scale = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "    \n",
    "        latents = torch.zeros(\n",
    "            (1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            device=\"cuda\",\n",
    "            dtype=main_dtype\n",
    "        )\n",
    "        \n",
    "        #true_noise = torch.randn_like(latents)\n",
    "\n",
    "        noises = torch.randn(\n",
    "            (run_context.steps, 1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            device=\"cuda\",\n",
    "            dtype=main_dtype\n",
    "        )\n",
    "\n",
    "        latents = step_by_noise(latents, true_noise[0], diffusion_timesteps[-1], diffusion_timesteps[0])\n",
    "        \n",
    "        original_size = (height, width)\n",
    "        target_size = (height, width)\n",
    "        crop_coords_top_left = (0, 0)\n",
    "\n",
    "        # incomprehensible var name tbh go read the sdxl paper if u want to Understand\n",
    "        add_time_ids = torch.tensor([list(original_size + crop_coords_top_left + target_size)], dtype=unet_dtype).repeat(unet_batch_size,1).to(\"cuda\")\n",
    "    \n",
    "        added_cond_kwargs = {\"text_embeds\": enc2_pooled.to(unet_dtype), \"time_ids\": add_time_ids}\n",
    "\n",
    "        \n",
    "        out_index = 0\n",
    "        with Timer(\"core loop\"):\n",
    "            #for step_index in range(run_context.steps)[::-1]:\n",
    "            #    latents = step_by_noise(latents, noises[step_index], diffusion_timesteps[step_index+1], diffusion_timesteps[step_index])\n",
    "            \n",
    "            for step_index in range(run_context.steps):\n",
    "                step_context = Context(run_context)\n",
    "                step_context.step_index = step_index\n",
    "                add_step_context(step_context)\n",
    "\n",
    "                #true_noise = noises[step_index]\n",
    "                #latents = step_by_noise(latents, true_noise, diffusion_timesteps[-1], diffusion_timesteps[step_index])\n",
    "                scaled_true_noise = true_noise#step_by_noise(torch.zeros_like(true_noise), true_noise, diffusion_timesteps[-1], diffusion_timesteps[step_index])\n",
    "                \n",
    "                def predict_noise(latents, step=diffusion_timesteps[step_index]):\n",
    "                    predictions = unet(\n",
    "                        latents.repeat(unet_batch_size, 1, 1, 1).to(unet_dtype),\n",
    "                        step, \n",
    "                        encoder_hidden_states=all_penult_states.to(unet_dtype),\n",
    "                        return_dict=False, \n",
    "                        added_cond_kwargs=added_cond_kwargs\n",
    "                    )[0]\n",
    "    \n",
    "                    return combine_predictions(step_context)(predictions.to(main_dtype), scaled_true_noise)\n",
    "\n",
    "                if method == \"euler\":\n",
    "                    final_prediction = predict_noise(latents)\n",
    "\n",
    "                if method == \"heun\":\n",
    "                    first_prediction = predict_noise(latents)\n",
    "\n",
    "                    tentative_result = step_by_noise(latents, first_prediction, diffusion_timesteps[step_index], diffusion_timesteps[step_index + 1])\n",
    "                    \n",
    "                    second_prediction = predict_noise(tentative_result, diffusion_timesteps[step_index + 1])\n",
    "    \n",
    "                    final_prediction = (first_prediction + second_prediction) / 2\n",
    "\n",
    "                if method == \"rk2\":\n",
    "                    first_prediction = predict_noise(latents)\n",
    "                    \n",
    "                    half_step = (diffusion_timesteps[step_index + 1] + diffusion_timesteps[step_index]) // 2\n",
    "                    \n",
    "                    tentative_result = step_by_noise(latents, first_prediction, diffusion_timesteps[step_index], half_step)\n",
    "                    \n",
    "                    final_prediction = predict_noise(tentative_result, half_step)\n",
    "\n",
    "                if method == \"rk4\":\n",
    "                    half_step = (diffusion_timesteps[step_index + 1] + diffusion_timesteps[step_index]) // 2\n",
    "                    \n",
    "                    prediction_1 = predict_noise(latents)\n",
    "                    result_1 = step_by_noise(latents, prediction_1, diffusion_timesteps[step_index], half_step)\n",
    "                    \n",
    "                    prediction_2 = predict_noise(result_1, half_step)\n",
    "                    result_2 = step_by_noise(latents, prediction_2, diffusion_timesteps[step_index], half_step)\n",
    "                    \n",
    "                    prediction_3 = predict_noise(result_2, half_step)\n",
    "                    result_3 = step_by_noise(latents, prediction_3, diffusion_timesteps[step_index], diffusion_timesteps[step_index + 1])\n",
    "                    \n",
    "                    prediction_4 = predict_noise(result_3, diffusion_timesteps[step_index + 1])\n",
    "\n",
    "                    final_prediction = (prediction_1 + 2 * (prediction_2 + prediction_3) + prediction_4) / 6\n",
    "                    \n",
    "\n",
    "                pred_original_sample = step_by_noise(latents, final_prediction, diffusion_timesteps[step_index], diffusion_timesteps[-1])\n",
    "                \n",
    "                latents = step_by_noise(latents, final_prediction, diffusion_timesteps[step_index], diffusion_timesteps[step_index + 1])\n",
    "\n",
    "                if save_raw(step_context):\n",
    "                    save_raw_latents(pred_original_sample)\n",
    "                if save_approximates(step_context):\n",
    "                    save_approx_decode(pred_original_sample, out_index)\n",
    "                    out_index += 1\n",
    "            \n",
    "            images_pil = pilify(pred_original_sample, vae)\n",
    "    \n",
    "            for im in images_pil:\n",
    "                display(im)\n",
    "\n",
    "            if save_output(run_context):\n",
    "                for n in range(len(images_pil)):\n",
    "                    images_pil[n].save(f\"{settings_directory}/{run_id}_final{n}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88aa0a1-7366-4fba-b1d0-e31d952a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save # # #\n",
    "\n",
    "Path(daily_directory).mkdir(exist_ok=True, parents=True)\n",
    "Path(f\"{daily_directory}/{settings_id}_{run_id}\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for n in range(len(images_pil)):\n",
    "    images_pil[n].save(f\"{daily_directory}/{settings_id}_{run_id}/{n}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fef9706-7a81-4786-9c5a-50eadb73217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pil = pilify(pred_original_sample * 5 -4, vae)\n",
    "    \n",
    "for im in images_pil:\n",
    "    display(im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
