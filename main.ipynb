{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92e3da-3717-4f30-a393-8a8828f06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import importlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, EulerDiscreteScheduler\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from autumn.notebook import *\n",
    "from autumn.math import *\n",
    "from autumn.images import *\n",
    "from autumn.prompting import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d8522-7bcc-4af7-bdd0-32ad43ca1abe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # # torch / cuda # # #\n",
    "\n",
    "DTYPE = torch.float16\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def save_raw_latents(latents):\n",
    "    lmin = latents.min()\n",
    "    l = latents - lmin\n",
    "    lmax = latents.max()\n",
    "    l = latents / lmax\n",
    "    l = l.float() / 2 + 0.5\n",
    "    l = l.detach().cpu().numpy() * 255\n",
    "    l = l.round().astype(\"uint8\")\n",
    "    \n",
    "\n",
    "    ims = []\n",
    "    \n",
    "    for lat in l:\n",
    "        row1 = np.concatenate([lat[0], lat[1]])\n",
    "        row2 = np.concatenate([lat[2], lat[3]])\n",
    "        grid = np.concatenate([row1, row2], axis=1)\n",
    "        #for channel in lat:\n",
    "        im = Image.fromarray(grid)\n",
    "        im = im.resize(size=(1024, 1024), resample=Image.NEAREST)\n",
    "        ims += [im]\n",
    "    \n",
    "    #clear_output()\n",
    "    #print(f\"normalized with coefficients  lmin={lmin}  lmax={lmax}\")\n",
    "    for im in ims:\n",
    "        im.save(\"out/tmp_raw_latents.png\")\n",
    "        #display(im)\n",
    "\n",
    "approximation_matrix = [\n",
    "    [0.85, 0.85, 0.6], # seems to be mainly value\n",
    "    [-0.35, 0.2, 0.5], # mainly blue? maybe a little green, def not red\n",
    "    [0.15, 0.15, 0], # yellow. but mainly encoding texture not color, i think\n",
    "    [0.15, -0.35, -0.35] # inverted value? but also red\n",
    "]\n",
    "\n",
    "def save_approx_decode(latents):\n",
    "    lmin = latents.min()\n",
    "    l = latents - lmin\n",
    "    lmax = latents.max()\n",
    "    l = latents / lmax\n",
    "    l = l.float() / 2 + 0.5\n",
    "    ims = []\n",
    "    for lat in l:\n",
    "        apx_mat = torch.tensor(approximation_matrix).to(\"cuda\")\n",
    "        approx_decode = torch.einsum(\"...lhw,lr -> ...rhw\", lat, apx_mat)\n",
    "        lat -= lat.min()\n",
    "        lat /= lat.max()\n",
    "        im_data = approx_decode.permute(1,2,0).detach().cpu().numpy() * 255\n",
    "        im_data = im_data.round().astype(\"uint8\")\n",
    "        im = Image.fromarray(im_data).resize(size=(1024,1024), resample=Image.NEAREST)\n",
    "        ims += [im]\n",
    "\n",
    "    #clear_output()\n",
    "    for im in ims:\n",
    "        im.save(\"out/tmp_approx_decode.png\")\n",
    "        #display(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a33fb-7751-45d9-a67b-3d7da6ca52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%settings\n",
    "\n",
    "model = \"model/name/goes/here\"\n",
    "XL_MODEL = True\n",
    "\n",
    "p0 = \"sixteenth century painting of a very cute dog rolling around in a grassy field, oil on canvas, neo-cubist glitch art, extremely detailed maya render of a fractal\"\n",
    "\n",
    "np0 = \"blurry, ugly, indistinct, jpeg artifacts, watermark, text, signature\"\n",
    "\n",
    "# Generally: e2_prompt has the strongest effect, but terms from e1_prompt will show up in results. e2_pool_prompts has an influence but very indirect idk\n",
    "\n",
    "# Prompts for encoder 1\n",
    "e1_prompts = [np0, p0]\n",
    "\n",
    "# Prompts for encoder 2; defaults to e1_prompts if None\n",
    "e2_prompts = None\n",
    "\n",
    "# Prompts for pooled encoding of encoder 2; defaults to e2_prompts if None\n",
    "e2_pool_prompts = None\n",
    "\n",
    "# Method by which predictions for different prompts will be recombined to make one noise prediction\n",
    "scale = 10\n",
    "combine_predictions = lambda p: p[0] + scale * (p[1] - p[0])\n",
    "\n",
    "seed = 42069\n",
    "\n",
    "# 16 => 1024\n",
    "height = 16\n",
    "width = 16\n",
    "\n",
    "steps = 25\n",
    "\n",
    "#distort = sigmoid(0.319, 0.987, 0.0304, 0.025)\n",
    "#distort = scale_f(distort, 127, 1.0)\n",
    "\n",
    "embedding_distortion = None\n",
    "\n",
    "#!# Settings above this line will be replaced with the contents of settings.py if it exists. #!#\n",
    "\n",
    "height *= 64;\n",
    "width *= 64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31d118-c74d-4693-a1d3-6f6383e854c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # # models # # #\n",
    "\n",
    "model_source = model\n",
    "\n",
    "with Timer(\"total\"):\n",
    "    with Timer(\"vae\"):\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            model_source, subfolder=\"vae\", torch_dtype=torch.float32\n",
    "        )\n",
    "        vae.to(device=\"cuda\")\n",
    "\n",
    "        # (as of torch 2.2.2) VAE compilation yields negligible gains, at least for decoding once/step\n",
    "        #vae = torch.compile(vae, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"unet\"):\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_source, subfolder=\"unet\", torch_dtype=DTYPE\n",
    "        )\n",
    "        unet.to(device=\"cuda\")\n",
    "    \n",
    "        # compilation will not actually happen until first use of unet\n",
    "        # (as of torch 2.2.2) \"default\" provides the best result on my machine\n",
    "        # don't use this if you're gonna be changing resolutions a lot\n",
    "        unet_c = torch.compile(unet, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"clip\"):\n",
    "        p_encoder = PromptEncoder(model_source, XL_MODEL, torch_dtype=DTYPE)\n",
    "    \n",
    "    with Timer(\"scheduler\"):\n",
    "        scheduler = EulerDiscreteScheduler.from_pretrained(\n",
    "            model, subfolder=\"scheduler\", torch_dtype=DTYPE\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24aeca-ec13-41cd-ae56-4a72c78dbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run # # #\n",
    "\n",
    "run_id = uuid.uuid4()\n",
    "\n",
    "try:\n",
    "    _seed = int(seed)\n",
    "except:\n",
    "    _seed = random.randint(1, 2**31 - 1)\n",
    "\n",
    "torch.manual_seed(_seed)\n",
    "np.random.seed(_seed)\n",
    "\n",
    "unet_batch_size = len(e1_prompts)\n",
    "\n",
    "(all_penult_states, enc2_pooled) = p_encoder.encode(e1_prompts, e2_prompts, e2_pool_prompts)\n",
    "\n",
    "if embedding_distortion is not None:\n",
    "    all_penult_states = svd_distort_embeddings(all_penult_states.to(torch.float32), embedding_distortion).to(torch.float16)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vae_scale = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "\n",
    "    latents = torch.randn(\n",
    "        (1, unet.config.in_channels, height // vae_scale, width // vae_scale),\n",
    "        device=\"cuda\",\n",
    "        dtype=DTYPE,\n",
    "    )\n",
    "\n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    #latents = vae.encode(latents.to(torch.float32)).latent_dist.sample().to(torch.float16)\n",
    "\n",
    "    original_size = (height, width)\n",
    "    target_size = (height, width)\n",
    "    crop_coords_top_left = (0, 0)\n",
    "\n",
    "    add_time_ids = list(original_size + crop_coords_top_left + target_size)\n",
    "\n",
    "    passed_add_embed_dim = (unet.config.addition_time_embed_dim * len(add_time_ids) + p_encoder.text_encoder_2.config.projection_dim)\n",
    "    expected_add_embed_dim = unet.add_embedding.linear_1.in_features\n",
    "\n",
    "    if passed_add_embed_dim != expected_add_embed_dim:\n",
    "        print(\"embed dim is messed up\")\n",
    "\n",
    "    add_time_ids = torch.tensor([add_time_ids], dtype=DTYPE).repeat(unet_batch_size,1).to(\"cuda\")\n",
    "\n",
    "    added_cond_kwargs = {\"text_embeds\": enc2_pooled, \"time_ids\": add_time_ids}\n",
    "\n",
    "    with Timer(\"core loop\"):\n",
    "        for step in scheduler.timesteps:\n",
    "            latents_expanded = latents.repeat(unet_batch_size, 1, 1, 1)\n",
    "            \n",
    "            latents_expanded = scheduler.scale_model_input(\n",
    "                latents_expanded, timestep=step\n",
    "            )\n",
    "\n",
    "            noise_prediction = unet(\n",
    "                latents_expanded, step, return_dict=False, encoder_hidden_states=all_penult_states,\n",
    "                added_cond_kwargs=added_cond_kwargs\n",
    "            )[0]\n",
    "            \n",
    "            predictions_split = noise_prediction.chunk(unet_batch_size)\n",
    "    \n",
    "            noise_prediction = combine_predictions(predictions_split)\n",
    "    \n",
    "            sched_out = scheduler.step(\n",
    "                #noise_prediction, step, svd_distort(latents.to(torch.float32), distort).to(torch.float16)\n",
    "                noise_prediction, step, latents\n",
    "            )\n",
    "            \n",
    "            latents = sched_out.prev_sample\n",
    "            save_raw_latents(sched_out.pred_original_sample)\n",
    "            save_approx_decode(sched_out.pred_original_sample)\n",
    "        \n",
    "        images_pil = PILify(sched_out.pred_original_sample, vae)\n",
    "\n",
    "        #clear_output()\n",
    "        for im in images_pil:\n",
    "            display(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbeded8-e406-4973-92d6-d728d3c3a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save # # #\n",
    "\n",
    "Path(daily_directory).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for n in range(len(images_pil)):\n",
    "    images_pil[n].save(f\"{daily_directory}/{settings_id}_{n}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6680d35-7cf9-4721-a963-8d0788f2d0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
