{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92e3da-3717-4f30-a393-8a8828f06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import importlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as func\n",
    "import numpy as np\n",
    "import transformers\n",
    "from diffusers import UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from autumn.notebook import *\n",
    "from autumn.math import *\n",
    "from autumn.images import *\n",
    "from autumn.guidance import *\n",
    "from autumn.scheduling import *\n",
    "from autumn.solvers import *\n",
    "from autumn.prompting import PromptEncoder\n",
    "from autumn.vae import SDXL_VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a33fb-7751-45d9-a67b-3d7da6ca52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%settings\n",
    "\n",
    "# model can be a local path or a huggingface model name\n",
    "model = \"stabilityai/stable-diffusion-xl-base-0.9\"\n",
    "XL_MODEL = True # this notebook in its current state is only really guaranteed to work with SDXL\n",
    "vae_scale = 0.13025 # found in the model config\n",
    "\n",
    "run_ids = range(5)\n",
    "\n",
    "# run context will contain only context.run_id, anything returned in this dictionary will be added to it\n",
    "add_run_context = lambda context: {}\n",
    "\n",
    "p0 = \"photograph of a very cute dog\"\n",
    "#np0 = \"blurry, ugly, indistinct, jpeg artifacts, watermark, text, signature\"\n",
    "\n",
    "# Generally: e2_prompt has the strongest effect, but terms from e1_prompt will show up in results.\n",
    "# e2_pool_prompts has an influence but very indirect idk\n",
    "# the model is trained w/ all encodings coming from the same prompt\n",
    "\n",
    "prompts = lambda context: {\n",
    "    # Prompts for encoder 1\n",
    "    \"encoder_1\": [p0],\n",
    "    # Prompts for encoder 2; defaults to e1_prompts if None\n",
    "    \"encoder_2\": None,\n",
    "    # Prompts for pooled encoding of encoder 2; defaults to e2_prompts if None\n",
    "    \"encoder_2_pooled\": None\n",
    "}\n",
    "\n",
    "# Method by which predictions for different prompts will be recombined to make one noise prediction\n",
    "combine_predictions = lambda context: true_noise_removal(context, [1])\n",
    "\n",
    "seed = lambda context: 42069 + context.run_id\n",
    "\n",
    "# these get multiplied by 64\n",
    "width_height = lambda context: (16, 16)\n",
    "\n",
    "steps = lambda context: 15\n",
    "\n",
    "# for scaling by a sqrt or **2 curve, &c\n",
    "timestep_power = lambda c: 1\n",
    "timestep_max = lambda c: 999\n",
    "timestep_min = lambda c: 0\n",
    "\n",
    "# differential equation solver. see autumn/solvers.py\n",
    "solver_step = lambda c: rk4_step\n",
    "\n",
    "# step context will contain run_id & step_index, anything returned in this dictionary will be added to it\n",
    "add_step_context = lambda context: {}\n",
    "\n",
    "embedding_distortion = lambda context: None\n",
    "\n",
    "save_output = lambda context: True\n",
    "save_approximates = lambda context: False\n",
    "save_raw = lambda context: False\n",
    "\n",
    "#!# Settings above this line will be replaced with the contents of settings.py if it exists. #!#\n",
    "\n",
    "main_dtype = torch.float64\n",
    "unet_dtype = torch.float16\n",
    "vae_dtype = torch.float32\n",
    "encoder_dtype = torch.float16\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31d118-c74d-4693-a1d3-6f6383e854c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # models # # #\n",
    "\n",
    "model_source = model\n",
    "\n",
    "with Timer(\"total\"):\n",
    "    with Timer(\"vae\"):\n",
    "        vae = SDXL_VAE()\n",
    "        vae.load_safetensors(model_source)\n",
    "        vae.to(device=\"cuda\")\n",
    "    \n",
    "        # (as of torch 2.2.2) VAE compilation yields negligible gains, at least for decoding once/step\n",
    "        #vae = torch.compile(vae, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"unet\"):\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_source, subfolder=\"unet\", torch_dtype=unet_dtype\n",
    "        )\n",
    "        unet.to(device=\"cuda\")\n",
    "    \n",
    "        # compilation will not actually happen until first use of unet\n",
    "        # (as of torch 2.2.2) \"default\" provides the best result on my machine\n",
    "        # don't use this if you're gonna be changing resolutions a lot\n",
    "        unet_c = torch.compile(unet, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"clip\"):\n",
    "        p_encoder = PromptEncoder(model_source, XL_MODEL, torch_dtype=encoder_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24aeca-ec13-41cd-ae56-4a72c78dbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run # # #\n",
    "\n",
    "variance_range = (0.00085, 0.012) # should come from model config!\n",
    "forward_noise_schedule = default_variance_schedule(variance_range).to(main_dtype) # beta\n",
    "forward_signal_product = torch.cumprod((1 - forward_noise_schedule), dim=0) # alpha_bar\n",
    "partial_signal_product = lambda s, t: torch.prod((1 - forward_noise_schedule)[s+1:t]) # alpha_bar_t / alpha_bar_s (but computed more directly from the forward noise)\n",
    "part_noise = (1 - forward_signal_product).sqrt() # sigma\n",
    "part_signal = forward_signal_product.sqrt() # mu?\n",
    "\n",
    "def step_by_noise(latents, noise, from_timestep, to_timestep):\n",
    "    if from_timestep < to_timestep: # forward\n",
    "        signal_ratio = 1 / partial_signal_product(from_timestep, to_timestep).sqrt()\n",
    "    else: # backward\n",
    "        signal_ratio = partial_signal_product(to_timestep, from_timestep).sqrt()\n",
    "    return latents / signal_ratio + noise * (part_noise[to_timestep] - part_noise[from_timestep] / signal_ratio)\n",
    "\n",
    "def shuffle_step(latents, first_noise, second_noise, timestep, intermediate_timestep):\n",
    "    if from_timestep < to_timestep: # forward\n",
    "        signal_ratio = 1 / partial_signal_product(timestep, intermediate_timestep).sqrt()\n",
    "    else: # backward\n",
    "        signal_ratio = partial_signal_product(intermediate_timestep, timestep).sqrt()\n",
    "    return latents + (first_noise - second_noise) * (part_noise[intermediate_timestep] * signal_ratio - part_noise[timestep])\n",
    "\n",
    "for run_id in run_ids:\n",
    "    run_context = Context()\n",
    "    run_context.run_id = run_id\n",
    "    add_run_context(run_context)\n",
    "    \n",
    "    try:\n",
    "        _seed = int(seed(run_context))\n",
    "    except:\n",
    "        _seed = 0\n",
    "        print(f\"non-integer seed, run {run_id}. replaced with 0.\")\n",
    "    \n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "\n",
    "    run_context.steps = steps(run_context)\n",
    "\n",
    "    diffusion_timesteps = linspace_timesteps(run_context.steps+1, timestep_max(run_context), timestep_min(run_context), timestep_power(run_context))\n",
    "\n",
    "    run_prompts = prompts(run_context)\n",
    "    \n",
    "    unet_batch_size = len(run_prompts[\"encoder_1\"])\n",
    "    \n",
    "    (all_penult_states, enc2_pooled) = p_encoder.encode(run_prompts[\"encoder_1\"], run_prompts[\"encoder_2\"], run_prompts[\"encoder_2_pooled\"])\n",
    "\n",
    "    for index in range(all_penult_states.shape[0]):\n",
    "        run_context.embedding_index = index\n",
    "        if embedding_distortion(run_context) is not None:\n",
    "            all_penult_states[index] = svd_distort_embeddings(all_penult_states[index].to(main_dtype), embedding_distortion(run_context)).to(unet_dtype)\n",
    "\n",
    "    width, height = width_height(run_context)\n",
    "\n",
    "    if (width < 64): width *= 64\n",
    "    if (height < 64): height *= 64\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vae_dim_scale = 2 ** 3\n",
    "    \n",
    "        latents = torch.zeros(\n",
    "            (1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            device=\"cuda\",\n",
    "            dtype=main_dtype\n",
    "        )\n",
    "\n",
    "        modify_initial_latents(run_context, latents)\n",
    "        \n",
    "        noises = torch.randn(\n",
    "            #(run_context.steps, 1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            (1, 1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            device=\"cuda\",\n",
    "            dtype=main_dtype\n",
    "        )\n",
    "\n",
    "        latents = step_by_noise(latents, noises[0], diffusion_timesteps[-1], diffusion_timesteps[0])\n",
    "        \n",
    "        original_size = (height, width)\n",
    "        target_size = (height, width)\n",
    "        crop_coords_top_left = (0, 0)\n",
    "\n",
    "        # incomprehensible var name tbh go read the sdxl paper if u want to Understand\n",
    "        add_time_ids = torch.tensor([list(original_size + crop_coords_top_left + target_size)], dtype=unet_dtype).repeat(unet_batch_size,1).to(\"cuda\")\n",
    "    \n",
    "        added_cond_kwargs = {\"text_embeds\": enc2_pooled.to(unet_dtype), \"time_ids\": add_time_ids}\n",
    "\n",
    "        \n",
    "        out_index = 0\n",
    "        with Timer(\"core loop\"):\n",
    "            for step_index in range(run_context.steps):\n",
    "                step_context = Context(run_context)\n",
    "                step_context.step_index = step_index\n",
    "                add_step_context(step_context)\n",
    "\n",
    "                #lerp_term = (part_signal[diffusion_timesteps[step_index]] + part_signal[diffusion_timesteps[step_index+1]]) / 2\n",
    "                #step_context.sqrt_signal = part_signal[diffusion_timesteps[step_index+1]] ** 0.5\n",
    "                #step_context.pnoise = (1-part_noise[diffusion_timesteps[step_index+1]]) ** 0.5\n",
    "                #step_context.lerp_by_noise = lambda a, b: lerp(a, b, part_signal[diffusion_timesteps[step_index+1]] ** 0.5)\n",
    "\n",
    "                noise = noises[0]\n",
    "\n",
    "\n",
    "                start_timestep = index_interpolate(diffusion_timesteps, step_index).round().int()\n",
    "                end_timestep = index_interpolate(diffusion_timesteps, step_index + 1).round().int()\n",
    "\n",
    "                \n",
    "                step_context.sqrt_signal = part_signal[end_timestep] ** 0.5\n",
    "                step_context.pnoise = (1-part_noise[end_timestep]) ** 0.5\n",
    "                step_context.lerp_by_noise = lambda a, b: lerp(a, b, part_signal[end_timestep] ** 0.5)\n",
    "                \n",
    "                #latents = step_by_noise(latents, noise, diffusion_timesteps[-1], diffusion_timesteps[step_index])\n",
    "                #latents = step_by_noise(latents, noise, diffusion_timesteps[-1], start_timestep)\n",
    "                \n",
    "                def predict_noise(latents, step=0):\n",
    "                    predictions = unet(\n",
    "                        latents.repeat(unet_batch_size, 1, 1, 1).to(unet_dtype),\n",
    "                        index_interpolate(diffusion_timesteps, step_index + step).round().int(), \n",
    "                        encoder_hidden_states=all_penult_states.to(unet_dtype),\n",
    "                        return_dict=False, \n",
    "                        added_cond_kwargs=added_cond_kwargs\n",
    "                    )[0]\n",
    "    \n",
    "                    return combine_predictions(step_context)(predictions.to(main_dtype), noise)\n",
    "                \n",
    "                def standard_diffusion_step(latents, noise, start, end):\n",
    "                    start_timestep = index_interpolate(diffusion_timesteps, step_index + start).round().int()\n",
    "                    end_timestep = index_interpolate(diffusion_timesteps, step_index + end).round().int()\n",
    "                    return step_by_noise(latents, noise, start_timestep, end_timestep)\n",
    "\n",
    "                def cfgpp_diffusion_step(latents, start, end, lda):\n",
    "                    start_timestep = index_interpolate(diffusion_timesteps, step_index + start).round().int()\n",
    "                    end_timestep = index_interpolate(diffusion_timesteps, step_index + end).round().int()\n",
    "\n",
    "                    predictions = unet(\n",
    "                        latents.repeat(unet_batch_size, 1, 1, 1).to(unet_dtype),\n",
    "                        start_timestep, \n",
    "                        encoder_hidden_states=all_penult_states.to(unet_dtype),\n",
    "                        return_dict=False, \n",
    "                        added_cond_kwargs=added_cond_kwargs\n",
    "                    )[0]\n",
    "\n",
    "                    eps = predictions[0] + lda * (predictions[1] - predictions[0])\n",
    "                    \n",
    "                    x_c = (latents - part_noise[start_timestep] * eps) / part_signal[start_timestep]\n",
    "                    return part_signal[end_timestep] * x_c + part_noise[end_timestep] * predictions[0]\n",
    "\n",
    "                def cfg_diffusion_step(latents, start, end, cfg_scale):\n",
    "                    start_timestep = index_interpolate(diffusion_timesteps, step_index + start).round().int()\n",
    "                    end_timestep = index_interpolate(diffusion_timesteps, step_index + end).round().int()\n",
    "\n",
    "                    predictions = unet(\n",
    "                        latents.repeat(unet_batch_size, 1, 1, 1).to(unet_dtype),\n",
    "                        start_timestep, \n",
    "                        encoder_hidden_states=all_penult_states.to(unet_dtype),\n",
    "                        return_dict=False, \n",
    "                        added_cond_kwargs=added_cond_kwargs\n",
    "                    )[0]\n",
    "\n",
    "                    eps = predictions[0] + cfg_scale * (predictions[1] - predictions[0])\n",
    "                    \n",
    "                    x_c = (latents - part_noise[start_timestep] * eps) / part_signal[start_timestep]\n",
    "                    return part_signal[end_timestep] * x_c + part_noise[end_timestep] * eps\n",
    "\n",
    "                def foo_step(latents, start, end):\n",
    "                    noise_prediction = solver_step(step_context)(predict_noise, standard_diffusion_step, latents)\n",
    "                    return standard_diffusion_step(latents, noise_prediction, 0, 1)\n",
    "                \n",
    "                if method == \"foo\":\n",
    "                    latents = foo_step(latents, 0, 1)\n",
    "                if method == \"cfg++\":\n",
    "                    latents = cfgpp_diffusion_step(latents, 0, 1, lda(step_context))\n",
    "                if method == \"cfg\":\n",
    "                    latents = cfg_diffusion_step(latents, 0, 1, cfg_scale(step_context))\n",
    "                \n",
    "                if step_index < run_context.steps - 1:\n",
    "                    pred_original_sample = step_by_noise(latents, noise, diffusion_timesteps[step_index+1], diffusion_timesteps[-1])\n",
    "                    #pred_original_sample = step_by_noise(latents, noise, end_timestep, diffusion_timesteps[-1])\n",
    "                else:\n",
    "                    pred_original_sample = latents\n",
    "                \n",
    "                #latents = step_by_noise(pred_original_sample, noises[0], diffusion_timesteps[-1], diffusion_timesteps[step_index])\n",
    "                #latents = step_by_noise(latents, noises[0], diffusion_timesteps[-1], diffusion_timesteps[step_index])\n",
    "\n",
    "                #latents = pred_original_sample\n",
    "                \n",
    "                if save_raw(step_context):\n",
    "                    save_raw_latents(pred_original_sample)\n",
    "                if save_approximates(step_context):\n",
    "                    save_approx_decode(pred_original_sample, out_index)\n",
    "                    out_index += 1\n",
    "\n",
    "                #if step_index > run_context.steps - 4:\n",
    "\n",
    "            images_pil = pilify(pred_original_sample, vae)\n",
    "    \n",
    "            for im in images_pil:\n",
    "                display(im)\n",
    "\n",
    "            if save_output(run_context):\n",
    "                for n in range(len(images_pil)):\n",
    "                    images_pil[n].save(f\"{settings_directory}/{run_id}_final{n}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88aa0a1-7366-4fba-b1d0-e31d952a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save # # #\n",
    "\n",
    "Path(daily_directory).mkdir(exist_ok=True, parents=True)\n",
    "Path(f\"{daily_directory}/{settings_id}_{run_id}\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for n in range(len(images_pil)):\n",
    "    images_pil[n].save(f\"{daily_directory}/{settings_id}_{run_id}/{n}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
