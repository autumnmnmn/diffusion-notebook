{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92e3da-3717-4f30-a393-8a8828f06cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import importlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from autumn.notebook import *\n",
    "from autumn.math import *\n",
    "from autumn.images import *\n",
    "from autumn.guidance import *\n",
    "from autumn.scheduling import *\n",
    "from autumn.prompting import PromptEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d8522-7bcc-4af7-bdd0-32ad43ca1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # torch / cuda # # #\n",
    "\n",
    "main_dtype = torch.double\n",
    "unet_dtype = torch.float16\n",
    "vae_dtype = torch.float32\n",
    "encoder_dtype = torch.float32\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def save_raw_latents(latents):\n",
    "    lmin = latents.min()\n",
    "    l = latents - lmin\n",
    "    lmax = latents.max()\n",
    "    l = latents / lmax\n",
    "    l = l.float() * 127.5 + 127.5\n",
    "    l = l.detach().cpu().numpy()\n",
    "    l = l.round().astype(\"uint8\")\n",
    "    \n",
    "\n",
    "    ims = []\n",
    "    \n",
    "    for lat in l:\n",
    "        row1 = np.concatenate([lat[0], lat[1]])\n",
    "        row2 = np.concatenate([lat[2], lat[3]])\n",
    "        grid = np.concatenate([row1, row2], axis=1)\n",
    "        #for channel in lat:\n",
    "        im = Image.fromarray(grid)\n",
    "        im = im.resize(size=(grid.shape[1]*4, grid.shape[0]*4), resample=Image.NEAREST)\n",
    "        ims += [im]\n",
    "    \n",
    "    #clear_output()\n",
    "    #print(f\"normalized with coefficients  lmin={lmin}  lmax={lmax}\")\n",
    "    for im in ims:\n",
    "        im.save(\"out/tmp_raw_latents.bmp\")\n",
    "        #display(im)\n",
    "\n",
    "approximation_matrix = [\n",
    "    [0.85, 0.85, 0.6], # seems to be mainly value\n",
    "    [-0.35, 0.2, 0.5], # mainly blue? maybe a little green, def not red\n",
    "    [0.15, 0.15, 0], # yellow. but mainly encoding texture not color, i think\n",
    "    [0.15, -0.35, -0.35] # inverted value? but also red\n",
    "]\n",
    "\n",
    "def save_approx_decode(latents, index):\n",
    "    lmin = latents.min()\n",
    "    l = latents - lmin\n",
    "    lmax = latents.max()\n",
    "    l = latents / lmax\n",
    "    l = l.float().mul_(0.5).add_(0.5)\n",
    "    ims = []\n",
    "    for lat in l:\n",
    "        apx_mat = torch.tensor(approximation_matrix).to(\"cuda\")\n",
    "        approx_decode = torch.einsum(\"...lhw,lr -> ...rhw\", lat, apx_mat).mul_(255).round()\n",
    "        #lat -= lat.min()\n",
    "        #lat /= lat.max()\n",
    "        im_data = approx_decode.permute(1,2,0).detach().cpu().numpy().astype(\"uint8\")\n",
    "        #im_data = im_data.round().astype(\"uint8\")\n",
    "        im = Image.fromarray(im_data).resize(size=(im_data.shape[1]*8,im_data.shape[0]*8), resample=Image.NEAREST)\n",
    "        ims += [im]\n",
    "\n",
    "    #clear_output()\n",
    "    for im in ims:\n",
    "        #im.save(f\"out/tmp_approx_decode/{index:06d}.bmp\")\n",
    "        im.save(f\"out/tmp_approx_decode.bmp\")\n",
    "        #display(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a33fb-7751-45d9-a67b-3d7da6ca52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%settings\n",
    "\n",
    "# model can be a local path or a huggingface model name\n",
    "model = \"stabilityai/stable-diffusion-xl-base-0.9\"\n",
    "XL_MODEL = True # this notebook in its current state is only really guaranteed to work with SDXL\n",
    "vae_scale = 0.13025 # found in the model config\n",
    "\n",
    "run_ids = range(1)\n",
    "\n",
    "# run context will contain only context.run_id, anything returned in this dictionary will be added to it\n",
    "add_run_context = lambda context: {}\n",
    "\n",
    "p0 = \"photograph of a very cute dog\"\n",
    "np0 = \"blurry, ugly, indistinct, jpeg artifacts, watermark, text, signature\"\n",
    "\n",
    "# Generally: e2_prompt has the strongest effect, but terms from e1_prompt will show up in results.\n",
    "# e2_pool_prompts has an influence but very indirect idk\n",
    "# the model is trained w/ all encodings coming from the same prompt\n",
    "\n",
    "prompts = lambda context: {\n",
    "    # Prompts for encoder 1\n",
    "    \"encoder_1\": [np0, p0],\n",
    "    # Prompts for encoder 2; defaults to e1_prompts if None\n",
    "    \"encoder_2\": None,\n",
    "    # Prompts for pooled encoding of encoder 2; defaults to e2_prompts if None\n",
    "    \"encoder_2_pooled\": None\n",
    "}\n",
    "\n",
    "# Method by which predictions for different prompts will be recombined to make one noise prediction\n",
    "combine_predictions = lambda context: scaled_CFG(\n",
    "    difference_scales = [\n",
    "        (1, 0, sigmoid(0.1 * vae_scale, 3 * vae_scale))\n",
    "    ], \n",
    "    steering_scale = id_, base_scale = id_, total_scale = id_\n",
    ")\n",
    "\n",
    "seed = lambda context: 42069\n",
    "\n",
    "# these get multiplied by 64\n",
    "width_height = lambda context: (16, 16)\n",
    "\n",
    "steps = lambda context: 25\n",
    "\n",
    "# step context will contain run_id & step_index, anything returned in this dictionary will be added to it\n",
    "add_step_context = lambda context: {}\n",
    "\n",
    "#distort = sigmoid(0.319, 0.987, 0.0304, 0.025)\n",
    "#distort = scale_f(distort, 127, 1.0)\n",
    "embedding_distortion = lambda context: None\n",
    "\n",
    "dynamic_thresholding = lambda context: False\n",
    "dynthresh_percentile = lambda context: 0.995\n",
    "dynthresh_target = lambda context: 7\n",
    "\n",
    "naive_rescaling = lambda context: False\n",
    "\n",
    "save_output = lambda context: True\n",
    "save_approximates = lambda context: False\n",
    "save_raw = lambda context: False\n",
    "\n",
    "#!# Settings above this line will be replaced with the contents of settings.py if it exists. #!#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e31d118-c74d-4693-a1d3-6f6383e854c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # # models # # #\n",
    "\n",
    "model_source = model\n",
    "\n",
    "with Timer(\"total\"):\n",
    "    with Timer(\"vae\"):\n",
    "        vae = AutoencoderKL.from_pretrained(\n",
    "            model_source, subfolder=\"vae\", torch_dtype=vae_dtype\n",
    "        )\n",
    "        vae.to(device=\"cuda\")\n",
    "\n",
    "        # (as of torch 2.2.2) VAE compilation yields negligible gains, at least for decoding once/step\n",
    "        #vae = torch.compile(vae, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"unet\"):\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_source, subfolder=\"unet\", torch_dtype=unet_dtype\n",
    "        )\n",
    "        unet.to(device=\"cuda\")\n",
    "    \n",
    "        # compilation will not actually happen until first use of unet\n",
    "        # (as of torch 2.2.2) \"default\" provides the best result on my machine\n",
    "        # don't use this if you're gonna be changing resolutions a lot\n",
    "        unet_c = torch.compile(unet, mode=\"default\", fullgraph=True)\n",
    "    \n",
    "    with Timer(\"clip\"):\n",
    "        p_encoder = PromptEncoder(model_source, XL_MODEL, torch_dtype=encoder_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24aeca-ec13-41cd-ae56-4a72c78dbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run # # #\n",
    "\n",
    "variance_range = (0.00085, 0.012) # should come from model config!\n",
    "forward_noise_schedule = default_variance_schedule(variance_range).to(main_dtype) # beta\n",
    "forward_signal_product = torch.cumprod((1 - forward_noise_schedule), dim=0) # alpha_bar\n",
    "partial_signal_product = lambda s, t: torch.prod((1 - forward_noise_schedule)[s+1:t]) # alpha_bar_t / alpha_bar_s\n",
    "\n",
    "for run_id in run_ids:\n",
    "    run_context = Context()\n",
    "    run_context.run_id = run_id\n",
    "    add_run_context(run_context)\n",
    "    \n",
    "    try:\n",
    "        _seed = int(seed(run_context))\n",
    "    except:\n",
    "        _seed = 0\n",
    "        print(f\"non-integer seed, run {run_id}. replaced with 0.\")\n",
    "    \n",
    "    torch.manual_seed(_seed)\n",
    "    np.random.seed(_seed)\n",
    "\n",
    "    run_prompts = prompts(run_context)\n",
    "    \n",
    "    unet_batch_size = len(run_prompts[\"encoder_1\"])\n",
    "    \n",
    "    (all_penult_states, enc2_pooled) = p_encoder.encode(run_prompts[\"encoder_1\"], run_prompts[\"encoder_2\"], run_prompts[\"encoder_2_pooled\"])\n",
    "    \n",
    "    if embedding_distortion(run_context) is not None:\n",
    "        all_penult_states = svd_distort_embeddings(all_penult_states.to(torch.float32), embedding_distortion(run_context)).to(torch.float16)\n",
    "\n",
    "    width, height = width_height(run_context)\n",
    "\n",
    "    if (width < 64): width *= 64\n",
    "    if (height < 64): height *= 64\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vae_dim_scale = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "    \n",
    "        latents = torch.randn(\n",
    "            (1, unet.config.in_channels, height // vae_dim_scale, width // vae_dim_scale),\n",
    "            device=\"cuda\",\n",
    "            dtype=main_dtype,\n",
    "        )\n",
    "\n",
    "        _steps = steps(run_context)\n",
    "        run_context.steps = _steps\n",
    "    \n",
    "        diffusion_timesteps = torch.arange(999, 0, -1000/_steps).round().int()\n",
    "        \n",
    "        step_signal_product = torch.from_numpy(np.interp(diffusion_timesteps, np.arange(0, len(forward_signal_product)), forward_signal_product))\n",
    "        \n",
    "        original_size = (height, width)\n",
    "        target_size = (height, width)\n",
    "        crop_coords_top_left = (0, 0)\n",
    "    \n",
    "        add_time_ids = list(original_size + crop_coords_top_left + target_size)\n",
    "    \n",
    "        passed_add_embed_dim = (unet.config.addition_time_embed_dim * len(add_time_ids) + p_encoder.text_encoder_2.config.projection_dim)\n",
    "        expected_add_embed_dim = unet.add_embedding.linear_1.in_features\n",
    "    \n",
    "        if passed_add_embed_dim != expected_add_embed_dim:\n",
    "            print(\"embed dim is messed up\")\n",
    "    \n",
    "        add_time_ids = torch.tensor([add_time_ids], dtype=unet_dtype).repeat(unet_batch_size,1).to(\"cuda\")\n",
    "    \n",
    "        added_cond_kwargs = {\"text_embeds\": enc2_pooled.to(unet_dtype), \"time_ids\": add_time_ids}\n",
    "    \n",
    "        out_index = 0\n",
    "        with Timer(\"core loop\"):\n",
    "            for step_index in range(_steps):\n",
    "                step_context = Context(run_context)\n",
    "                step_context.step_index = step_index\n",
    "                step_context.diffusion_timestep = diffusion_timesteps[step_index]\n",
    "                add_step_context(step_context)\n",
    "                \n",
    "                latents_expanded = latents.repeat(unet_batch_size, 1, 1, 1)\n",
    "\n",
    "                noise_prediction = unet(\n",
    "                    latents_expanded.to(unet_dtype), \n",
    "                    step_context.diffusion_timestep, \n",
    "                    encoder_hidden_states=all_penult_states.to(unet_dtype),\n",
    "                    return_dict=False, \n",
    "                    added_cond_kwargs=added_cond_kwargs\n",
    "                )[0]\n",
    "                \n",
    "                predictions_split = noise_prediction.to(main_dtype).chunk(unet_batch_size)\n",
    "    \n",
    "                noise_prediction = combine_predictions(step_context)(predictions_split)\n",
    "    \n",
    "                if dynamic_thresholding(step_context):\n",
    "                    apply_dynthresh(predictions_split, noise_prediction, dynthresh_target(step_context), dynthresh_percentile(step_context))\n",
    "    \n",
    "                if naive_rescaling(step_context):\n",
    "                    apply_naive_rescale(predictions_split, noise_prediction)\n",
    "                \n",
    "                current_part_noise = (1 - step_signal_product[step_index]).sqrt()\n",
    "                current_part_signal = step_signal_product[step_index].sqrt()\n",
    "                next_part_noise = 0\n",
    "                signal_ratio = 1\n",
    "                if step_index < _steps - 1:\n",
    "                    next_part_noise = (1 - step_signal_product[step_index + 1]).sqrt()\n",
    "                    signal_ratio = partial_signal_product(diffusion_timesteps[step_index+1], diffusion_timesteps[step_index]).sqrt()\n",
    "\n",
    "                pred_original_sample = (latents - current_part_noise * noise_prediction) / current_part_signal\n",
    "                \n",
    "                prev_sample = latents / signal_ratio + noise_prediction * (next_part_noise - current_part_noise / signal_ratio)\n",
    "                \n",
    "                latents = prev_sample\n",
    "                \n",
    "                if save_raw(step_context):\n",
    "                    save_raw_latents(pred_original_sample)\n",
    "                if save_approximates(step_context):\n",
    "                    save_approx_decode(pred_original_sample, out_index)\n",
    "                    out_index += 1\n",
    "            \n",
    "            images_pil = pilify(pred_original_sample, vae)\n",
    "    \n",
    "            for im in images_pil:\n",
    "                display(im)\n",
    "\n",
    "            if save_output(run_context):\n",
    "                for n in range(len(images_pil)):\n",
    "                    images_pil[n].save(f\"{settings_directory}/{run_id}_final{n}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a3a99-319e-41b4-a1bd-f7e26383a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_pil = pilify(sigmoid(5, 2 / vae_scale)(sched_out.pred_original_sample), vae)\n",
    "    \n",
    "for im in images_pil:\n",
    "    display(im)\n",
    "\n",
    "images_pil = pilify(sigmoid(6, 2 / vae_scale)(sched_out.pred_original_sample), vae)\n",
    "    \n",
    "for im in images_pil:\n",
    "    display(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88aa0a1-7366-4fba-b1d0-e31d952a6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save # # #\n",
    "\n",
    "Path(daily_directory).mkdir(exist_ok=True, parents=True)\n",
    "Path(f\"{daily_directory}/{settings_id}_{run_id}\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for n in range(len(images_pil)):\n",
    "    images_pil[n].save(f\"{daily_directory}/{settings_id}_{run_id}/{n}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
